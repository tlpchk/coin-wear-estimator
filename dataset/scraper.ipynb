{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('coin-wear-env')",
   "metadata": {
    "interpreter": {
     "hash": "9214e9ed1261b8386bff29a139da31c318f4d999a62610e883c248e0fae4a597"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from urllib.parse import urlparse, urlunparse, urlencode\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper(ABC):\n",
    "    def __init__(self, base_url):\n",
    "        self._base_url = base_url\n",
    "        self._driver = webdriver.Chrome()\n",
    "\n",
    "    @abstractmethod\n",
    "    def open_search_page(self, keyword: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_list_of_coin_urls(self, keyword: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def scrap_coin_data(self, link: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def scrap_coin_data_from_page(self, link):\n",
    "        pass\n",
    "\n",
    "    def link_of_coins_to_csv(self, keyword, csv_path):\n",
    "        links = self.get_list_of_coin_urls(keyword)\n",
    "        df = pd.DataFrame({\"link\": links})\n",
    "        df.to_csv(csv_path, header=None, index=None)\n",
    "        return df\n",
    "   \n",
    "    def scroll_to_bottom(self):\n",
    "        self._driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    def quit(self):\n",
    "        self._driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiemsczykScraper(Scraper):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"https://archiwum.niemczyk.pl\")\n",
    "\n",
    "    def open_search_page(self, keyword: str, page = None):\n",
    "        url = urlparse(self._base_url + \"/search\")\n",
    "        params = {\"search\": keyword}\n",
    "        if page:\n",
    "            params['page'] = page\n",
    "        url_new_query = urlencode(params)\n",
    "        url_parsed = url._replace(query=url_new_query)\n",
    "        url = urlunparse(url_parsed)\n",
    "        self._driver.get(url)\n",
    "\n",
    "    def get_list_of_coin_urls(self, keyword: str):\n",
    "        driver = self._driver\n",
    "        all_links = []\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        page = 1\n",
    "        self.open_search_page(keyword, page)\n",
    "        try:\n",
    "            while wait.until(EC.visibility_of_element_located((By.ID, \"post-data\"))):\n",
    "                page_links = list(map(lambda el : el.get_attribute(\"href\"), driver.find_elements_by_xpath(\"//div[@id='post-data']//h3/a\")))\n",
    "                all_links += page_links\n",
    "                page = page + 1\n",
    "                self.open_search_page(keyword, page)\n",
    "        except TimeoutException as error:\n",
    "            print(\"Fetched {} links\".format(len(all_links)))\n",
    "        return all_links\n",
    "\n",
    "    def scrap_coin_data_from_page(self, link):\n",
    "        driver = self._driver\n",
    "        driver.get(link)\n",
    "        date = driver.find_element_by_xpath(\"//div[text()='Data sprzeda≈ºy:']/../div[2]\").text\n",
    "        title = driver.find_element_by_xpath(\"//div[@class='row']//h2/span[1]\").text\n",
    "        try:\n",
    "            images = list(map(lambda el : el.get_attribute(\"href\"), driver.find_elements_by_xpath(\"//div[@id='links']/a\")))\n",
    "        except Exception as e:\n",
    "            images = []\n",
    "        try: \n",
    "            desctiption = driver.find_element_by_xpath(\"//div[@class='tab-content']//div[@class='container']\").text\n",
    "        except Exception as e:\n",
    "            desctiption = \"\"\n",
    "        \n",
    "        desctiption = desctiption.replace(\"\\n\",\"\\\\\")\n",
    "        return { \"title\": title, \"date\": date, \"description\": desctiption, \"images\": images, \"link\": link }\n",
    "    \n",
    "    def scrap_coin_data(self, links, csv_path):\n",
    "        df = pd.DataFrame()\n",
    "        for link in links:\n",
    "            data = self.scrap_coin_data_from_page(link)\n",
    "            df = df.append(data, ignore_index=True)\n",
    "        df.to_csv(csv_path, index=None)\n",
    "        return df\n",
    "\n",
    "    def scrap_coin_data_dev(self, links, csv_path):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, sep='|')\n",
    "        except:\n",
    "            df = pd.DataFrame()\n",
    "        for link in links:\n",
    "            data = self.scrap_coin_data_from_page(link)\n",
    "            df = df.append(data, ignore_index=True)\n",
    "            df.to_csv(csv_path, index=None, sep='|')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niemczyk.quit()\n",
    "niemczyk = NiemsczykScraper()\n",
    "\n",
    "keyword = \"Sztandar 1930\"\n",
    "filename = \"{}.csv\".format(keyword.lower().replace(\" \",\"_\"))\n",
    "links_filename = \"niemczyk/links/\" + filename \n",
    "data_filename = \"niemczyk/data/\" + filename\n",
    "\n",
    "niemczyk.link_of_coins_to_csv(keyword, links_filename)\n",
    "df = pd.read_csv(links_filename, header=None)\n",
    "niemczyk.scrap_coin_data_dev(list(df[0])[334:], data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarciniakScraper(Scraper):\n",
    "    def __init__(self, ):\n",
    "        super().__init__(\"https://aukcje.gndm.pl/pl/archive\")\n",
    "\n",
    "    def open_search_page(self, keyword: str, page = 1):\n",
    "        url = urlparse(\"{}/{}/0/0/0/{}\".format(self._base_url, page, keyword))\n",
    "        url = urlunparse(url)\n",
    "        self._driver.get(url)\n",
    "\n",
    "\n",
    "    def get_list_of_coin_urls(self, keyword: str):\n",
    "        driver = self._driver\n",
    "        all_links = []\n",
    "        page = 1\n",
    "        wait = WebDriverWait(driver, 3)\n",
    "       \n",
    "        while True:\n",
    "            self.open_search_page(keyword, page)\n",
    "            i = 1\n",
    "            try:\n",
    "                while wait.until(EC.visibility_of_element_located((By.ID, \"scrollpart\" + str(i)))):\n",
    "                    scrollpart_links = list(map(lambda el : el.get_attribute(\"href\"),\n",
    "                                                driver.find_elements_by_xpath(\"//div[@id='scrollpart{}']//a[@class='title']\".format(i))\n",
    "                                                ))\n",
    "                    all_links += scrollpart_links\n",
    "                    self.scroll_to_bottom()\n",
    "                    i = i + 1\n",
    "            except TimeoutException as error:\n",
    "                if i != 11:\n",
    "                    break\n",
    "            finally:\n",
    "                page += 1\n",
    "\n",
    "        print(\"Fetched {} links\".format(len(all_links)))\n",
    "        return all_links\n",
    "\n",
    "    def scrap_coin_data(self, link: str):\n",
    "        pass\n",
    "\n",
    "    def scrap_coin_data_from_page(self, link):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marciniak.quit()\n",
    "marciniak = MarciniakScraper()\n",
    "\n",
    "keyword = \"Sztandar 1930\"\n",
    "filename = \"{}.csv\".format(keyword.lower().replace(\" \",\"_\"))\n",
    "links_filename = \"marciniak/links/\" + filename \n",
    "data_filename = \"marciniak/data/\" + filename\n",
    "\n",
    "marciniak.link_of_coins_to_csv(keyword, links_filename)"
   ]
  }
 ]
}